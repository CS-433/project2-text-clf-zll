{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPkKqMC/ax8cwRiz3WG9o0H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ot69iVdwh1dT","executionInfo":{"status":"ok","timestamp":1670619936065,"user_tz":-60,"elapsed":4688,"user":{"displayName":"李心蔚","userId":"16297085280211430075"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","from torchtext.data import get_tokenizer\n","from torch.utils.data import Dataset, DataLoader\n","from torchtext.datasets import IMDB\n","from collections import Counter, OrderedDict\n","from torchtext.vocab import vocab, GloVe\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from sklearn.metrics import precision_recall_fscore_support\n","import csv"]},{"cell_type":"markdown","source":[],"metadata":{"id":"wNdrMhJcKXXg"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ir_F2X-Di_qS","executionInfo":{"status":"ok","timestamp":1670619956308,"user_tz":-60,"elapsed":20247,"user":{"displayName":"李心蔚","userId":"16297085280211430075"}},"outputId":"7397ecb0-3135-43e7-cdde-3ccc98ecb23a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["pos_file_path = \"/content/drive/MyDrive/Colab Notebooks/twitter-datasets/train_pos.txt\"\n","neg_file_path = \"/content/drive/MyDrive/Colab Notebooks/twitter-datasets/train_neg.txt\"\n","test_file_path = \"/content/drive/MyDrive/Colab Notebooks/twitter-datasets/test_data.txt\"\n","pos_file_path_prep = \"/content/drive/MyDrive/Colab Notebooks/twitter-datasets/train_pos_prep.txt\"\n","neg_file_path_prep = \"/content/drive/MyDrive/Colab Notebooks/twitter-datasets/train_neg_prep.txt\"\n","test_file_path_prep = \"/content/drive/MyDrive/Colab Notebooks/twitter-datasets/test_data_prep.txt\"\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/output/'"],"metadata":{"id":"GgEcZWzKixjr","executionInfo":{"status":"ok","timestamp":1670619956308,"user_tz":-60,"elapsed":4,"user":{"displayName":"李心蔚","userId":"16297085280211430075"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Load training data\n","with open(pos_file_path, encoding='utf8') as f:\n","# with open(pos_file_path_prep, encoding='utf8') as f:\n","    content = f.readlines()\n","data = [x.strip() for x in content]\n","data_pos = pd.DataFrame(data)\n","\n","with open(neg_file_path, encoding='utf8') as f:\n","# with open(neg_file_path_prep, encoding='utf8') as f:\n","    content = f.readlines()\n","data = [x.strip() for x in content]\n","data_neg = pd.DataFrame(data)\n","\n","data_pos['label'] = 1\n","data_neg['label'] = 0\n","data_all = pd.concat([data_neg,data_pos], axis=0)\n","data_all = data_all.sample(frac=1).reset_index()\n","data_all = data_all.rename(columns = {0:'tweet'})\n","data_all = data_all.drop(['index'],axis=1)\n","\n","# Load submission data\n","with open(test_file_path, encoding='utf8') as f:\n","# with open(test_file_path_prep, encoding='utf8') as f:\n","    content = f.readlines()\n","data = [x.strip() for x in content]\n","submission_data = pd.DataFrame(data)\n","\n","submission_data = submission_data.rename(columns = {0:'tweet'})\n","submission_data['label'] = 1\n","\n","training_size = int(0.9 * len(data_all))\n"],"metadata":{"id":"4n08OkAGh5Xe","executionInfo":{"status":"ok","timestamp":1670619958658,"user_tz":-60,"elapsed":2354,"user":{"displayName":"李心蔚","userId":"16297085280211430075"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\n","# Create dataset\n","class TextClassificationDataset(Dataset):\n","    def __init__(self, data, tokenizer, text_vocab, label_vocab, split='train'):\n","        print(f'Numericalising tokens for {split} set...', end=\"\", flush=True)\n","        self.dset = []\n","        self.labels = []\n","        self.text_vocab = text_vocab\n","        self.label_vocab = label_vocab\n","        for tweet, label in data:\n","            tokens = tokenizer(tweet.rstrip())\n","            self.dset.append([self.text_vocab[w] for w in tokens])\n","            self.labels.append(self.label_vocab[str(label)])\n","        print(f'Number of {split} samples: {len(self.dset)}')\n","\n","    def __len__(self):\n","        return len(self.dset)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.dset[idx]\n","        label = self.labels[idx]\n","        return tokens, label\n","\n","    def tokens(self):\n","        return self.text_vocab.get_itos()\n","\n","    def vocab_size(self):\n","        return len(self.text_vocab)\n","\n","    def num_classes(self):\n","        return len(self.label_vocab)\n","\n","    @classmethod\n","    def build_vocab(cls, data, tokenizer, split='train', min_freq=1, pad_token='<pad>', unk_token='<unk>'):\n","        print(f'Building vocab for twitter...', end=\"\", flush=True)\n","        tokens = []\n","        labels = []\n","        for tweet, label in data:\n","            tokens += tokenizer(tweet.rstrip())\n","            labels.append(str(label))\n","\n","        def create_vocab(counts, mf=1):\n","            counter = Counter(counts)\n","            sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n","            ordered_dict = OrderedDict(sorted_by_freq_tuples)\n","            return vocab(ordered_dict, min_freq=mf)\n","\n","        text_vocab = create_vocab(tokens, min_freq)\n","        # set index for padding token\n","        text_vocab.insert_token(pad_token, 0)\n","        # set default index for unknown token\n","        text_vocab.append_token(unk_token)\n","        text_vocab.set_default_index(text_vocab[unk_token])\n","        label_vocab =  create_vocab(labels)\n","        print('Done!')\n","        print(f'Number of tokens: {len(text_vocab)}')\n","        print(f'Classes: {label_vocab.get_stoi()}')\n","        return cls(data, tokenizer, text_vocab, label_vocab, split)\n","    \n","seed = 123\n","torch.manual_seed(seed)\n","tokenizer = get_tokenizer(\"basic_english\")"],"metadata":{"id":"fWreWIz9h8yX","executionInfo":{"status":"ok","timestamp":1670619958659,"user_tz":-60,"elapsed":3,"user":{"displayName":"李心蔚","userId":"16297085280211430075"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["train_data=data_all[0:training_size]\n","test_data=data_all[training_size:]\n","train_set = TextClassificationDataset.build_vocab(train_data.values, tokenizer, split='train', min_freq=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gV4DfNGSo3xf","executionInfo":{"status":"ok","timestamp":1670619968794,"user_tz":-60,"elapsed":8016,"user":{"displayName":"李心蔚","userId":"16297085280211430075"}},"outputId":"9d046b75-f74b-4dd2-eba2-e600a9ffd015"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Building vocab for twitter...Done!\n","Number of tokens: 44080\n","Classes: {'0': 1, '1': 0}\n","Numericalising tokens for train set...Number of train samples: 180000\n"]}]},{"cell_type":"code","source":["vec = GloVe(name='6B', dim=100)\n","embeddings = vec.get_vecs_by_tokens(train_set.tokens(), lower_case_backup=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N-JfLSLKiEcR","executionInfo":{"status":"ok","timestamp":1670620312943,"user_tz":-60,"elapsed":325737,"user":{"displayName":"李心蔚","userId":"16297085280211430075"}},"outputId":"02dafe78-6014-4179-cce8-457cbd9f789b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":[".vector_cache/glove.6B.zip: 862MB [04:52, 2.95MB/s]                           \n","100%|█████████▉| 399999/400000 [00:14<00:00, 27053.85it/s]\n"]}]},{"cell_type":"code","source":["class AttentionModel(nn.Module):\n","    def __init__(\n","            self,\n","            vocab_size,\n","            num_label,\n","            e_dim=100,\n","            num_layer=1,\n","            num_head=2,\n","            dropout=0.1,\n","            max_len=512,\n","            padding_idx=0,\n","            weights=None,\n","            freeze=True,\n","            device=torch.device('cuda:0')):\n","\n","        super().__init__()\n","        self.padding_idx = padding_idx\n","        self.max_len = max_len\n","        if weights is None:\n","            self.word_embeddings = nn.Embedding(vocab_size, e_dim, padding_idx=padding_idx)\n","            if freeze:\n","                self.word_embeddings.weight.requires_grad = False\n","        else:\n","            self.word_embeddings = nn.Embedding.from_pretrained(weights, freeze=freeze, padding_idx=padding_idx)\n","        self.position_embeddings = nn.Embedding(max_len, e_dim)\n","        self.position_ids = torch.arange(max_len).to(device)\n","        self.LayerNorm = nn.LayerNorm(e_dim, eps=1e-5)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        transformer_layer = nn.TransformerEncoderLayer(d_model=e_dim, nhead=num_head, dim_feedforward=e_dim * 4)\n","        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_layer)\n","        self.classifier = nn.Linear(e_dim, num_label)\n","        self.device = device\n","        self.to(device)\n","\n","    def forward(self, inputs):\n","        token_ids, attn_masks = inputs\n","        batch_size = token_ids.size(0)\n","        batch_max_length = token_ids.size(1)\n","        # compute embeddings\n","        token_embs = self.word_embeddings(token_ids)\n","        pos_embs = self.position_embeddings(self.position_ids[:batch_max_length])\n","        embeddings = token_embs + pos_embs\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        # compute contextualised embeddings with transformer\n","        contextualised_embs = self.transformer(embeddings.permute(1, 0, 2), src_key_padding_mask=attn_masks)\n","        outputs = contextualised_embs.mean(0)\n","        logits = self.classifier(outputs)\n","        return logits\n","\n","    def knn(self, token_ids, k=10):\n","        query = self.word_embeddings.weight[token_ids]\n","        x_src = F.normalize(query)\n","        x_tgt = F.normalize(self.word_embeddings.weight)\n","        # compute cosine similarity\n","        scores = x_src @ x_tgt.t()\n","        top_values, top_indices = torch.topk(scores, k + 1)\n","        return top_indices[:, 1:]  # remove top1 since it is the target token\n","\n","    def collate_batch(self, batch):\n","        label_list, text_list, lengths = [], [], []\n","        for (_text, _label) in batch:\n","            label_list.append(_label)\n","            text_list.append(_text)\n","            lengths.append(len(_text))\n","        max_length = min(max(lengths), self.max_len)\n","        # truncate or add padding to the right hand side\n","        for i, _text in enumerate(text_list):\n","            if len(_text) < max_length:  # pad\n","                text_list[i] += [self.padding_idx] * (max_length - len(_text))\n","            else:  # truncate\n","                text_list[i] = _text[:max_length]\n","        label_list = torch.tensor(label_list, dtype=torch.long).to(self.device)\n","        text_list = torch.tensor(text_list, dtype=torch.long).to(self.device)\n","        attn_mask = text_list == self.padding_idx\n","        return label_list, (text_list, attn_mask)\n","\n","model = AttentionModel(\n","    vocab_size=train_set.vocab_size(), \n","    num_label=train_set.num_classes(), \n","    e_dim=100,         \n","    max_len=512,\n","    num_head=2,\n","    num_layer=1,\n","    weights=embeddings,\n","    freeze='store_true',\n","    padding_idx=train_set.text_vocab['<pad>'],    \n","    device=torch.device('cuda:0'))\n","\n","@torch.no_grad()\n","def word_knn(model, valid_set, vocab, top=10):\n","    print('--------------------------------------------------------------------------------')\n","    print('Validation - top 10 nearest neighbors ------------------------------------------')\n","    valid_token_ids = vocab.lookup_indices(valid_set)\n","    top_indices = model.knn(valid_token_ids, top)\n","\n","    for i, word in enumerate(valid_set):\n","        results = ' '.join([vocab.lookup_token(top_indices[i, k].item()) for k in range(top)])\n","        print(f'{word}: {results}')\n","    print('--------------------------------------------------------------------------------')\n","\n","\n","# define the loss function\n","ce_loss = nn.CrossEntropyLoss()\n","\n","# define the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","train_dataloader = DataLoader(train_set, \n","                            batch_size=512,                 #\n","                            shuffle=True, \n","                            collate_fn=model.collate_batch)\n","\n","\n","# loop over training epochs\n","EPOCHS = 40\n","for epoch in range(1, EPOCHS + 1):\n","    pbar = tqdm(train_dataloader)\n","    pbar.set_description(\"[Epoch {}]\".format(epoch))\n","    for labels, inputs in pbar: \n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = ce_loss(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        # display the loss \n","        pbar.set_postfix(loss=loss.item())\n","\n","\n","\n","# validation\n","test_set = TextClassificationDataset(\n","    test_data.values,\n","    tokenizer, \n","    text_vocab=train_set.text_vocab, \n","    label_vocab=train_set.label_vocab,\n","    split='test')\n","\n","test_dataloader = DataLoader(test_set, \n","                            batch_size=512,              \n","                            shuffle=False, \n","                            collate_fn=model.collate_batch)\n","\n","y_true = []\n","y_pred = []\n","\n","with torch.no_grad():\n","    model.eval()\n","    for labels, inputs in tqdm(test_dataloader, desc='[Validation]'):\n","        logits = model(inputs)\n","        y_pred += logits.argmax(dim=1).tolist()\n","        y_true += labels.tolist()\n","\n","pre, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n","\n","print('------- Evaluation metrics --------')\n","print(f'Precision: {pre*100:.2f}%')\n","print(f'Recall: {rec*100:.2f}%')\n","print(f'F1 score: {f1*100:.2f}%')\n","print('-' * 35)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1eiRkF2oiL_e","executionInfo":{"status":"ok","timestamp":1670620685407,"user_tz":-60,"elapsed":372466,"user":{"displayName":"李心蔚","userId":"16297085280211430075"}},"outputId":"1a081e00-96fb-4b32-94ba-6c09f630de5d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["[Epoch 1]: 100%|██████████| 352/352 [00:09<00:00, 37.71it/s, loss=0.481]\n","[Epoch 2]: 100%|██████████| 352/352 [00:08<00:00, 40.40it/s, loss=0.522]\n","[Epoch 3]: 100%|██████████| 352/352 [00:08<00:00, 39.43it/s, loss=0.523]\n","[Epoch 4]: 100%|██████████| 352/352 [00:09<00:00, 38.84it/s, loss=0.502]\n","[Epoch 5]: 100%|██████████| 352/352 [00:08<00:00, 39.57it/s, loss=0.468]\n","[Epoch 6]: 100%|██████████| 352/352 [00:08<00:00, 39.35it/s, loss=0.466]\n","[Epoch 7]: 100%|██████████| 352/352 [00:09<00:00, 39.04it/s, loss=0.458]\n","[Epoch 8]: 100%|██████████| 352/352 [00:09<00:00, 38.89it/s, loss=0.438]\n","[Epoch 9]: 100%|██████████| 352/352 [00:09<00:00, 38.60it/s, loss=0.426]\n","[Epoch 10]: 100%|██████████| 352/352 [00:09<00:00, 38.31it/s, loss=0.458]\n","[Epoch 11]: 100%|██████████| 352/352 [00:09<00:00, 38.46it/s, loss=0.433]\n","[Epoch 12]: 100%|██████████| 352/352 [00:09<00:00, 38.48it/s, loss=0.437]\n","[Epoch 13]: 100%|██████████| 352/352 [00:10<00:00, 33.17it/s, loss=0.499]\n","[Epoch 14]: 100%|██████████| 352/352 [00:09<00:00, 39.03it/s, loss=0.412]\n","[Epoch 15]: 100%|██████████| 352/352 [00:09<00:00, 39.06it/s, loss=0.403]\n","[Epoch 16]: 100%|██████████| 352/352 [00:08<00:00, 39.12it/s, loss=0.384]\n","[Epoch 17]: 100%|██████████| 352/352 [00:09<00:00, 38.69it/s, loss=0.46]\n","[Epoch 18]: 100%|██████████| 352/352 [00:09<00:00, 36.53it/s, loss=0.431]\n","[Epoch 19]: 100%|██████████| 352/352 [00:09<00:00, 37.10it/s, loss=0.409]\n","[Epoch 20]: 100%|██████████| 352/352 [00:09<00:00, 38.76it/s, loss=0.429]\n","[Epoch 21]: 100%|██████████| 352/352 [00:09<00:00, 36.90it/s, loss=0.403]\n","[Epoch 22]: 100%|██████████| 352/352 [00:09<00:00, 38.65it/s, loss=0.43]\n","[Epoch 23]: 100%|██████████| 352/352 [00:09<00:00, 38.91it/s, loss=0.384]\n","[Epoch 24]: 100%|██████████| 352/352 [00:09<00:00, 38.98it/s, loss=0.444]\n","[Epoch 25]: 100%|██████████| 352/352 [00:09<00:00, 39.07it/s, loss=0.344]\n","[Epoch 26]: 100%|██████████| 352/352 [00:09<00:00, 38.92it/s, loss=0.436]\n","[Epoch 27]: 100%|██████████| 352/352 [00:09<00:00, 38.86it/s, loss=0.348]\n","[Epoch 28]: 100%|██████████| 352/352 [00:09<00:00, 38.98it/s, loss=0.387]\n","[Epoch 29]: 100%|██████████| 352/352 [00:09<00:00, 38.34it/s, loss=0.443]\n","[Epoch 30]: 100%|██████████| 352/352 [00:09<00:00, 38.54it/s, loss=0.371]\n","[Epoch 31]: 100%|██████████| 352/352 [00:09<00:00, 38.78it/s, loss=0.373]\n","[Epoch 32]: 100%|██████████| 352/352 [00:09<00:00, 38.79it/s, loss=0.4]\n","[Epoch 33]: 100%|██████████| 352/352 [00:09<00:00, 38.84it/s, loss=0.383]\n","[Epoch 34]: 100%|██████████| 352/352 [00:09<00:00, 38.79it/s, loss=0.371]\n","[Epoch 35]: 100%|██████████| 352/352 [00:09<00:00, 38.36it/s, loss=0.38]\n","[Epoch 36]: 100%|██████████| 352/352 [00:09<00:00, 38.85it/s, loss=0.366]\n","[Epoch 37]: 100%|██████████| 352/352 [00:09<00:00, 37.59it/s, loss=0.378]\n","[Epoch 38]: 100%|██████████| 352/352 [00:09<00:00, 37.85it/s, loss=0.398]\n","[Epoch 39]: 100%|██████████| 352/352 [00:09<00:00, 38.00it/s, loss=0.409]\n","[Epoch 40]: 100%|██████████| 352/352 [00:09<00:00, 38.94it/s, loss=0.341]"]},{"output_type":"stream","name":"stdout","text":["Numericalising tokens for test set..."]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Number of test samples: 20000\n"]},{"output_type":"stream","name":"stderr","text":["[Validation]: 100%|██████████| 40/40 [00:00<00:00, 179.74it/s]"]},{"output_type":"stream","name":"stdout","text":["------- Evaluation metrics --------\n","Precision: 81.88%\n","Recall: 81.74%\n","F1 score: 81.71%\n","-----------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# create classification for test data\n","submission_set = TextClassificationDataset(\n","    submission_data.values,\n","    tokenizer, \n","    text_vocab=train_set.text_vocab, \n","    label_vocab=train_set.label_vocab,\n","    split='train')\n","\n","submission_dataloader = DataLoader(submission_set, \n","                            batch_size=256, \n","                            shuffle=False, \n","                            collate_fn=model.collate_batch)\n","\n","y_pred=[]\n","with torch.no_grad():\n","    model.eval()\n","    for labels, inputs in tqdm(submission_dataloader, desc='[Testing]'):\n","        logits = model(inputs)\n","        y_pred += logits.argmax(dim=1).tolist()\n"],"metadata":{"id":"I0hJcUuu0qrr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670621695459,"user_tz":-60,"elapsed":914,"user":{"displayName":"李心蔚","userId":"16297085280211430075"}},"outputId":"1bf36666-abe5-4765-9966-437715df3d73"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Numericalising tokens for train set...Number of train samples: 10000\n"]},{"output_type":"stream","name":"stderr","text":["[Testing]: 100%|██████████| 40/40 [00:00<00:00, 224.71it/s]\n"]}]},{"cell_type":"code","source":["# create submission\n","def create_csv_submission(ids, y_pred, name):\n","    \"\"\"\n","    Creates an output file in .csv format for submission to Kaggle or AIcrowd\n","    Arguments: ids (event ids associated with each prediction)\n","            y_pred (predicted class labels)\n","            name (string name of .csv output file to be created)\n","    \"\"\"\n","    with open(name, 'w') as csvfile:\n","        fieldnames = ['Id', 'Prediction']\n","        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n","        writer.writeheader()\n","        for r1, r2 in zip(ids, y_pred):\n","            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n","\n","y_pred = np.array(y_pred)*(-2)+1\n","y_pred = list(y_pred)\n","create_csv_submission(np.arange(1, len(y_pred)+1), y_pred, output_dir+'submission_transformer_2.csv')\n","y_pred"],"metadata":{"id":"ghR6PxjWlWPq","executionInfo":{"status":"ok","timestamp":1670621701880,"user_tz":-60,"elapsed":338,"user":{"displayName":"李心蔚","userId":"16297085280211430075"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"683639b4-d271-4df3-f06d-1e5993933b53"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," -1,\n"," -1,\n"," 1,\n"," 1,\n"," 1,\n"," -1,\n"," 1,\n"," ...]"]},"metadata":{},"execution_count":23}]}]}