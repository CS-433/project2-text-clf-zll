{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"b05d79cc-0040-41e2-83ec-5ba86dd246ac"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import jieba\n","from sklearn.model_selection import train_test_split\n","import pickle\n","import torch.nn.functional as F\n","import pandas as pd"],"id":"b05d79cc-0040-41e2-83ec-5ba86dd246ac"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3444,"status":"ok","timestamp":1670929319113,"user":{"displayName":"李展","userId":"09735672056344794134"},"user_tz":-60},"id":"4QxBZOAZLKfC","outputId":"f64940b7-14db-4a19-8b0a-2d0723f5e54d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting hiddenlayer\n","  Downloading hiddenlayer-0.3-py3-none-any.whl (19 kB)\n","Installing collected packages: hiddenlayer\n","Successfully installed hiddenlayer-0.3\n"]}],"source":["!pip install hiddenlayer"],"id":"4QxBZOAZLKfC"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20449,"status":"ok","timestamp":1670929339558,"user":{"displayName":"李展","userId":"09735672056344794134"},"user_tz":-60},"id":"gUpd3s7QtoFT","outputId":"cad17679-fe3c-4ad2-d060-9a5cf7b5aa88"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive \n","drive.mount('/content/drive')"],"id":"gUpd3s7QtoFT"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b71a6e97-b8ad-447d-bd51-8e62e84f9f44"},"outputs":[],"source":["def text_to_index_array(p_new_dic, tweets_list): \n","    '''\n","    Mapping text data to index matrix\n","    '''\n","    new_tweets = []\n","    for tweet in tweets_list:\n","        new_tweet = []\n","        temp = tweet.replace(\"<user>\", \"\").replace(\"\\n\", \"\").replace(\"<url>\", \"\").split()\n","        for word in temp:\n","            try:\n","                new_tweet.append(p_new_dic[word]) \n","            except:\n","                new_tweet.append(0)  # Set to 0 if not present in the vocabulary\n","        new_tweets.append(new_tweet)\n","    return np.array(new_tweets,dtype=object)   \n","\n","def text_cut_to_same_long(tweets_list):\n","    '''\n","    Cut the data to the same specified length  \n","    '''\n","    data_num = len(tweets_list)\n","    new_ = np.zeros((data_num,maxlen)) \n","    se = []\n","    for i in range(len(tweets_list)):\n","        new_[i,:] = tweets_list[i,:maxlen]        \n","    new_ = np.array(new_, dtype=object)\n","    return new_\n","    \n","def creat_wordvec_tensor(embedding_weights,X_T):\n","    '''\n","    Map the index matrix into a word vector matrix\n","    '''\n","    X_tt = np.zeros((len(X_T),maxlen,vocab_dim))\n","    num1 = 0\n","    num2 = 0\n","    for j in X_T:\n","        for i in j:\n","            X_tt[num1,num2,:] = embedding_weights[int(i),:]\n","            num2 = num2+1\n","        num1 = num1+1\n","        num2 = 0\n","    return X_tt"],"id":"b71a6e97-b8ad-447d-bd51-8e62e84f9f44"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0f7ed7d-64a4-495b-9a53-50a2cfd441f3"},"outputs":[],"source":["vocab_dim = 100\n","maxlen = 20  # Maximum length of text retention\n","   \n","embedding_weights = np.load(\"/content/drive/MyDrive/Colab Notebooks/D2V/Word2VecArray.npy\") \n","# Set a zero vector for words that do not appear in the vocabulary\n","embedding_weights = np.r_[np.zeros((1, vocab_dim)),embedding_weights]\n","\n","f = open(\"/content/drive/MyDrive/Colab Notebooks/Glove/vocab.pkl\", 'rb') \n","index_dict = pickle.load(f)    # index dictionary {'word': idx}\n","\n","# Index each word + 1 because of the zero vector\n","for key, value in index_dict.items():  \n","    index_dict[key] = value + 1 \n","\n","pos_ = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/part_pos.csv\")\n","neg_ = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/part_neg.csv\")\n","\n","pos_ = pos_.sample(frac=0.5, replace=True, random_state=1)\n","neg_ = neg_.sample(frac=0.5, replace=True, random_state=1)\n","\n","pos_data = pos_['tweet'].tolist()\n","neg_data = neg_['tweet'].tolist()\n","data = neg_data + pos_data\n","\n","label_list = ([0] * len(neg_data) + [1] * len(pos_data))"],"id":"e0f7ed7d-64a4-495b-9a53-50a2cfd441f3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9b7ecf8c-db02-444a-a8ba-2a8ba4c4c1ef"},"outputs":[],"source":["####LSTM####\n","train_x,val_x,train_y,val_y = train_test_split(data, label_list, test_size=0.05)\n","train_x = text_to_index_array(index_dict, train_x)\n","val_x = text_to_index_array(index_dict, val_x)\n","train_y = np.array(train_y) \n","val_y = np.array(val_y)"],"id":"9b7ecf8c-db02-444a-a8ba-2a8ba4c4c1ef"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54826,"status":"ok","timestamp":1670929416243,"user":{"displayName":"李展","userId":"09735672056344794134"},"user_tz":-60},"id":"66d50aa6-2995-43b7-9040-5a336b3d0688","outputId":"6d75efd0-1136-4471-f8bd-5c2b4a6470c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["train shape：  (1076160, 20, 100)\n","val shape：  (56641, 20, 100)\n"]}],"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","# Cut the data to the same specified length \n","train_x = pad_sequence([torch.from_numpy(np.array(x)) for x in train_x],batch_first=True).float() \n","val_x = pad_sequence([torch.from_numpy(np.array(x)) for x in val_x],batch_first=True).float()\n","train_x = text_cut_to_same_long(train_x)\n","val_x = text_cut_to_same_long(val_x)\n","\n","# Index to vector\n","train_x = creat_wordvec_tensor(embedding_weights,train_x)\n","print(\"train shape： \", train_x.shape)\n","val_x = creat_wordvec_tensor(embedding_weights,val_x)\n","print(\"val shape： \", val_x.shape)"],"id":"66d50aa6-2995-43b7-9040-5a336b3d0688"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8c57dd9-27dd-40a0-b4af-9ca6bf1b1359"},"outputs":[],"source":["batch_size = 128\n","\n","from torch.utils.data import TensorDataset, DataLoader\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n","test_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n","\n","train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n","  "],"id":"e8c57dd9-27dd-40a0-b4af-9ca6bf1b1359"},{"cell_type":"code","source":["    \n","class BiLSTM_Attention(nn.Module):\n","    def __init__(self):\n","\n","        super(BiLSTM_Attention, self).__init__()\n","        self.hidden_size = 128\n","        self.input_size = vocab_dim\n","        self.layer_size = 3\n","        self.lstm = nn.LSTM(self.input_size,\n","                            self.hidden_size,\n","                            self.layer_size,\n","                            batch_first=True,\n","                            bidirectional=True\n","                            )\n","        self.out = nn.Linear(self.hidden_size*2, 2)\n","\n","    def attention_net(self,lstm_output, final_state):\n","        # lstm_output : [batch_size, n_step, n_hidden * num_directions(=2)], F matrix\n","        # final_state : [num_layers(=2) * num_directions(=2), batch_size, n_hidden]\n","        batch_size = len(lstm_output)\n","        # hidden = final_state.view(batch_size,-1,1)\n","        hidden = torch.cat((final_state[0],final_state[1]),dim=1).unsqueeze(2)\n","        # hidden : [batch_size, n_hidden * num_directions(=2), n_layer(=2)]\n","        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2)\n","        soft_attn_weights = F.softmax(attn_weights,1)\n","\n","        # context: [batch_size, n_hidden * num_directions(=2)]\n","        context = torch.bmm(lstm_output.transpose(1,2),soft_attn_weights.unsqueeze(2)).squeeze(2)\n","\n","        return context, soft_attn_weights\n","\n","    def forward(self, input):\n","\n","        output, (final_hidden_state, final_cell_state) = self.lstm(input)\n","\n","        attn_output, attention = self.attention_net(output,final_hidden_state)\n","        return self.out(attn_output),attention # attn_output : [batch_size, num_classes], attention : [batch_size, n_step]\n","\n","     \n","class lstm(nn.Module):\n","    def __init__(self):\n","        super(lstm, self).__init__()\n","        self.lstm = nn.LSTM(\n","            input_size=vocab_dim,\n","            hidden_size=128,\n","            num_layers=3,\n","            batch_first=True)   \n","\n","    def forward(self, x):\n","        out, (h_0, c_0) = self.lstm(x)\n","        out = out[:, -1, :]\n","        out = self.fc(out)\n","        out = torch.sigmoid(out)    \n","        return out, h_0    "],"metadata":{"id":"IO9RjEuiV1t2"},"id":"IO9RjEuiV1t2","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a547eee2-f073-4a84-8658-e23b24802fe5","outputId":"2ee09c97-50d2-41fc-a76b-9e2dc81cf5ac","executionInfo":{"status":"ok","timestamp":1670932065038,"user_tz":-60,"elapsed":786773,"user":{"displayName":"李展","userId":"09735672056344794134"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["————————train————————\n","epoch:0 accuracy：78.637% loss = 0.43717319579647906\n","epoch:1 accuracy：81.982% loss = 0.3850613633466691\n","epoch:2 accuracy：83.589% loss = 0.35752925055932705\n","epoch:3 accuracy：84.775% loss = 0.3356568609425474\n","epoch:4 accuracy：85.713% loss = 0.318131183477014\n","epoch:5 accuracy：86.838% loss = 0.298056583163939\n","epoch:6 accuracy：88.103% loss = 0.2747761102341874\n","epoch:7 accuracy：89.466% loss = 0.24894800868601344\n","epoch:8 accuracy：90.773% loss = 0.22251136908192476\n","epoch:9 accuracy：91.914% loss = 0.1976707916588059\n","epoch:10 accuracy：92.935% loss = 0.17544059872274578\n"]}],"source":["####------train---------####\n","from sklearn.metrics import accuracy_score, classification_report\n","import hiddenlayer as hl\n","\n","model = BiLSTM_Attention()\n","model = model.to(device)\n","optimizer = torch.optim.Adam(model.parameters())\n","logStep=75\n","n_epoch = 11\n","\n","print ('————————train————————')\n","history1=hl.History()\n","canvas1=hl.Canvas()\n","\n","for epoch in range(n_epoch):\n","    correct = 0\n","    total = 0\n","    epoch_loss = 0\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):        \n","\n","        data = torch.as_tensor(data, dtype=torch.float32)\n","        target = target.long()   \n","        optimizer.zero_grad()\n","        data,target = data.cuda(),target.cuda()  \n","        output, h_state = model(data)\n","        #labels = output.argmax(dim= 1)\n","        #acc = accuracy_score(target, labels)\n","        \n","        correct += int(torch.sum(torch.argmax(output, dim=1) == target))\n","        total += len(target)\n","        \n","        optimizer.zero_grad()\n","        loss = F.cross_entropy(output, target) \n","        epoch_loss += loss.item()\n","        loss.backward() \n","        optimizer.step()\n","\n","        # niter=epoch*len(train_loader)+batch_idx+1\n","        # if niter % logStep ==0:\n","        #     val_x = torch.as_tensor(val_x, dtype=torch.float32)\n","        #     val_x = val_x.cuda() \n","        #     output,_= model(val_x)\n","        #     pre_lab=torch.argmax(output, dim=1)\n","        #     test_accuracy=accuracy_score(val_y,pre_lab.cpu())\n","        #     history1.log(niter,train_loss=loss,test_accuracy=test_accuracy)\n","        #     with canvas1:\n","        #         canvas1.draw_plot(history1['train_loss'])\n","        #         canvas1.draw_plot(history1['test_accuracy'])\n","    \n","    loss = epoch_loss / (batch_idx + 1)\n","    print ('epoch:%s'%epoch, 'accuracy：%.3f%%'%(correct *100 / total), 'loss = %s'%loss)\n","    "],"id":"a547eee2-f073-4a84-8658-e23b24802fe5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5WB_zgHMXRF","executionInfo":{"status":"ok","timestamp":1670932088076,"user_tz":-60,"elapsed":4632,"user":{"displayName":"李展","userId":"09735672056344794134"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c4299a34-7c34-47a1-e9f6-3d7dca069a5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["————————validation————————\n","epoch:0 accuracy：86.995% loss = 0.31199511697276183\n"]}],"source":["####------validation---------####\n","print ('————————validation————————')\n","# model = torch.load('/content/drive/MyDrive/Colab Notebooks/W2V_BiLSTM_attn2.pt')\n","for epoch in range(1):\n","    correct = 0\n","    total = 0\n","    epoch_loss = 0\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(test_loader):        \n","        #print (data.shape)\n","       \n","        data = torch.as_tensor(data, dtype=torch.float32)\n","        target = target.long()   \n","        optimizer.zero_grad()\n","        data,target = data.cuda(),target.cuda() \n","        output, h_state = model(data)\n","        #labels = output.argmax(dim= 1)\n","        #acc = accuracy_score(target, labels)\n","        \n","        correct += int(torch.sum(torch.argmax(output, dim=1) == target))\n","        total += len(target)\n","        \n","        optimizer.zero_grad()\n","        loss = F.cross_entropy(output, target)\n","        epoch_loss += loss.item()\n","        loss.backward() \n","        optimizer.step()\n","    \n","    loss = epoch_loss / (batch_idx + 1)\n","    print ('epoch:%s'%epoch, 'accuracy：%.3f%%'%(correct *100 / total), 'loss = %s'%loss)"],"id":"Q5WB_zgHMXRF"},{"cell_type":"code","source":["torch.save(model, '/content/drive/MyDrive/Colab Notebooks/W2V_BiLSTM_attn0.pt')"],"metadata":{"id":"IpZr_isROACz"},"id":"IpZr_isROACz","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"M0WXkVpsMM9q"},"id":"M0WXkVpsMM9q","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}