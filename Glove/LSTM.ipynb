{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b05d79cc-0040-41e2-83ec-5ba86dd246ac",
   "metadata": {
    "id": "b05d79cc-0040-41e2-83ec-5ba86dd246ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from text_process_function.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import import_ipynb\n",
    "from text_process_function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f7ed7d-64a4-495b-9a53-50a2cfd441f3",
   "metadata": {
    "id": "e0f7ed7d-64a4-495b-9a53-50a2cfd441f3"
   },
   "outputs": [],
   "source": [
    "vocab_dim = 20 \n",
    "maxlen = 40  # Maximum length of text retention\n",
    "   \n",
    "embedding_weights = np.load(\"embeddings.npy\") \n",
    "# Set a zero vector for words that do not appear in the vocabulary\n",
    "embedding_weights = np.r_[np.zeros((1, vocab_dim)),embedding_weights]\n",
    "\n",
    "f = open(\"vocab.pkl\", 'rb') \n",
    "index_dict = pickle.load(f)    # index dictionary {'word': idx}\n",
    "\n",
    "# Index each word + 1 because of the zero vector\n",
    "for key, value in index_dict.items():  \n",
    "    index_dict[key] = value + 1 \n",
    "\n",
    "with open(\"../twitter-datasets/train_neg.txt\", \"r\", encoding='UTF-8') as f:\n",
    "    neg_data = f.readlines()\n",
    "with open(\"../twitter-datasets/train_pos.txt\", \"r\", encoding='UTF-8') as f:\n",
    "    pos_data = f.readlines()\n",
    "    \n",
    "data = neg_data + pos_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b3f21a4-51ec-48ec-9785-f0241aad6294",
   "metadata": {
    "id": "8b3f21a4-51ec-48ec-9785-f0241aad6294"
   },
   "outputs": [],
   "source": [
    "label_list = ([0] * len(neg_data) + [1] * len(pos_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b7ecf8c-db02-444a-a8ba-2a8ba4c4c1ef",
   "metadata": {
    "id": "9b7ecf8c-db02-444a-a8ba-2a8ba4c4c1ef"
   },
   "outputs": [],
   "source": [
    "####LSTM####\n",
    "train_x,val_x,train_y,val_y = train_test_split(data, label_list, test_size=0.2)\n",
    "train_x = text_to_index_array(index_dict, train_x)\n",
    "val_x = text_to_index_array(index_dict, val_x)\n",
    "train_y = np.array(train_y) \n",
    "val_y = np.array(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66d50aa6-2995-43b7-9040-5a336b3d0688",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66d50aa6-2995-43b7-9040-5a336b3d0688",
    "outputId": "a385de14-718c-470e-a1ef-b2e0d7d34850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape：  (160000, 40, 20)\n",
      "val shape：  (40000, 40, 20)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Cut the data to the same specified length \n",
    "train_x = pad_sequence([torch.from_numpy(np.array(x)) for x in train_x],batch_first=True).float() \n",
    "val_x = pad_sequence([torch.from_numpy(np.array(x)) for x in val_x],batch_first=True).float()\n",
    "train_x = text_cut_to_same_long(train_x)\n",
    "val_x = text_cut_to_same_long(val_x)\n",
    "\n",
    "# Index to vector\n",
    "train_x = creat_wordvec_tensor(embedding_weights,train_x)\n",
    "val_x = creat_wordvec_tensor(embedding_weights,val_x)\n",
    "\n",
    "print(\"train shape： \", train_x.shape)\n",
    "print(\"val shape： \", val_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c57dd9-27dd-40a0-b4af-9ca6bf1b1359",
   "metadata": {
    "id": "e8c57dd9-27dd-40a0-b4af-9ca6bf1b1359"
   },
   "outputs": [],
   "source": [
    "n_epoch = 60 \n",
    "batch_size = 64 \n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "test_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "class lstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(lstm, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=vocab_dim,\n",
    "            hidden_size=64,\n",
    "            batch_first=True)   \n",
    "                                  \n",
    "        self.fc = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, (h_0, c_0) = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        out = torch.sigmoid(out)    \n",
    "        return out, h_0\n",
    "\n",
    "model = lstm()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a547eee2-f073-4a84-8658-e23b24802fe5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a547eee2-f073-4a84-8658-e23b24802fe5",
    "outputId": "609ad269-2508-4528-a6a0-ea24a03d597f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————train————————\n",
      "epoch:0 accuracy：57.522% loss = 0.6667917513370514\n",
      "epoch:1 accuracy：62.572% loss = 0.6408646491765976\n",
      "epoch:2 accuracy：63.430% loss = 0.6354870833158494\n",
      "epoch:3 accuracy：63.774% loss = 0.6325310456991196\n",
      "epoch:4 accuracy：63.113% loss = 0.6363121729612351\n",
      "epoch:5 accuracy：64.098% loss = 0.6288930665016175\n",
      "epoch:6 accuracy：64.226% loss = 0.6247829536676407\n",
      "epoch:7 accuracy：64.694% loss = 0.6197969586849212\n",
      "epoch:8 accuracy：65.730% loss = 0.6140653542876243\n",
      "epoch:9 accuracy：66.809% loss = 0.6077569638848305\n",
      "epoch:10 accuracy：67.877% loss = 0.6007283889532089\n",
      "epoch:11 accuracy：68.668% loss = 0.5944177234292031\n",
      "epoch:12 accuracy：69.793% loss = 0.5874420775175094\n",
      "epoch:13 accuracy：70.544% loss = 0.5823871362805366\n",
      "epoch:14 accuracy：71.093% loss = 0.5774488172292709\n",
      "epoch:15 accuracy：71.772% loss = 0.5722881619691849\n",
      "epoch:16 accuracy：72.332% loss = 0.5675608823418617\n",
      "epoch:17 accuracy：72.967% loss = 0.5626050394058227\n",
      "epoch:18 accuracy：73.412% loss = 0.5597102617263794\n",
      "epoch:19 accuracy：73.899% loss = 0.5557539964079857\n",
      "epoch:20 accuracy：74.314% loss = 0.552932642185688\n",
      "epoch:21 accuracy：74.832% loss = 0.5480358121156692\n",
      "epoch:22 accuracy：75.076% loss = 0.5463702106952667\n",
      "epoch:23 accuracy：75.433% loss = 0.5431516949892045\n",
      "epoch:24 accuracy：75.907% loss = 0.5395764558553696\n",
      "epoch:25 accuracy：76.243% loss = 0.5361879019618034\n",
      "epoch:26 accuracy：76.497% loss = 0.5346026777267456\n",
      "epoch:27 accuracy：76.934% loss = 0.5308840704202652\n",
      "epoch:28 accuracy：77.161% loss = 0.5292300707697868\n",
      "epoch:29 accuracy：77.391% loss = 0.5274810955166817\n",
      "epoch:30 accuracy：77.647% loss = 0.5248976336598397\n",
      "epoch:31 accuracy：77.942% loss = 0.5229065920591355\n",
      "epoch:32 accuracy：78.067% loss = 0.521665366089344\n",
      "epoch:33 accuracy：78.356% loss = 0.5194100977301598\n",
      "epoch:34 accuracy：78.551% loss = 0.5182216115593911\n",
      "epoch:35 accuracy：78.755% loss = 0.5160442489743233\n",
      "epoch:36 accuracy：78.764% loss = 0.5152027218103409\n",
      "epoch:37 accuracy：79.121% loss = 0.5125662012338639\n",
      "epoch:38 accuracy：79.266% loss = 0.5110163079977036\n",
      "epoch:39 accuracy：79.340% loss = 0.5106524502635003\n",
      "epoch:40 accuracy：79.481% loss = 0.5102522089362145\n",
      "epoch:41 accuracy：79.713% loss = 0.5082051953554153\n",
      "epoch:42 accuracy：79.927% loss = 0.5061617763519287\n",
      "epoch:43 accuracy：79.914% loss = 0.5058842542290688\n",
      "epoch:44 accuracy：80.005% loss = 0.5056503849983215\n",
      "epoch:45 accuracy：80.144% loss = 0.5043612547636032\n",
      "epoch:46 accuracy：80.313% loss = 0.5024181340098381\n",
      "epoch:47 accuracy：80.413% loss = 0.5015326198816299\n",
      "epoch:48 accuracy：80.434% loss = 0.5013111187100411\n",
      "epoch:49 accuracy：80.599% loss = 0.5004639836668968\n",
      "epoch:50 accuracy：80.812% loss = 0.4983566439986229\n",
      "epoch:51 accuracy：80.724% loss = 0.4988154676198959\n",
      "epoch:52 accuracy：80.879% loss = 0.4979560658812523\n",
      "epoch:53 accuracy：81.142% loss = 0.49535056265592575\n",
      "epoch:54 accuracy：81.208% loss = 0.4951515570282936\n",
      "epoch:55 accuracy：81.234% loss = 0.4943741518735886\n",
      "epoch:56 accuracy：81.414% loss = 0.4934060924887657\n",
      "epoch:57 accuracy：81.346% loss = 0.49356118763685225\n",
      "epoch:58 accuracy：81.488% loss = 0.4924716736555099\n",
      "epoch:59 accuracy：81.310% loss = 0.4935686099648476\n"
     ]
    }
   ],
   "source": [
    "####------train---------####\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print ('————————train————————')\n",
    "for epoch in range(n_epoch):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):        \n",
    "\n",
    "        data = torch.as_tensor(data, dtype=torch.float32)\n",
    "        target = target.long()   \n",
    "        optimizer.zero_grad()\n",
    "        # data,target = data.cuda(),target.cuda()  \n",
    "        output, h_state = model(data)\n",
    "        #labels = output.argmax(dim= 1)\n",
    "        #acc = accuracy_score(target, labels)\n",
    "        \n",
    "        correct += int(torch.sum(torch.argmax(output, dim=1) == target))\n",
    "        total += len(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(output, target) \n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "    \n",
    "    loss = epoch_loss / (batch_idx + 1)\n",
    "    print ('epoch:%s'%epoch, 'accuracy：%.3f%%'%(correct *100 / total), 'loss = %s'%loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1485903c-5b21-4c60-b9cd-3e387033fa11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1485903c-5b21-4c60-b9cd-3e387033fa11",
    "outputId": "b3bd0538-eb28-493f-c6dd-72798cf151a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————validation————————\n",
      "epoch:0 accuracy：71.890% loss = 0.5757300402641297\n"
     ]
    }
   ],
   "source": [
    "####------validation---------####\n",
    "print ('————————validation————————')\n",
    "for epoch in range(1):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):        \n",
    "        #print (data.shape)\n",
    "       \n",
    "        data = torch.as_tensor(data, dtype=torch.float32)\n",
    "        target = target.long()   \n",
    "        optimizer.zero_grad()\n",
    "        # data,target = data.cuda(),target.cuda() \n",
    "        output, h_state = model(data)\n",
    "        #labels = output.argmax(dim= 1)\n",
    "        #acc = accuracy_score(target, labels)\n",
    "        \n",
    "        correct += int(torch.sum(torch.argmax(output, dim=1) == target))\n",
    "        total += len(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "    \n",
    "    loss = epoch_loss / (batch_idx + 1)\n",
    "    print ('epoch:%s'%epoch, 'accuracy：%.3f%%'%(correct *100 / total), 'loss = %s'%loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80a1ca35-8475-4997-9dfe-92921ae6d7a4",
   "metadata": {
    "id": "80a1ca35-8475-4997-9dfe-92921ae6d7a4"
   },
   "outputs": [],
   "source": [
    "torch.save(model, '/content/drive/MyDrive/Colab Notebooks/Glove/Glove_LSTM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q5WB_zgHMXRF",
   "metadata": {
    "id": "Q5WB_zgHMXRF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
