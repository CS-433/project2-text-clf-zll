{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca3f54a1-88e5-4774-96c0-4e04817c1f9e",
   "metadata": {
    "id": "ca3f54a1-88e5-4774-96c0-4e04817c1f9e"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f159e660-f256-4de7-b0b2-e26b113c67c3",
   "metadata": {
    "id": "f159e660-f256-4de7-b0b2-e26b113c67c3"
   },
   "outputs": [],
   "source": [
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    randomlist = random.sample(range(1,len(data)),n)\n",
    "    return data[randomlist], labels[randomlist]\n",
    "\n",
    "def text_to_index_array(p_new_dic, tweets_list): \n",
    "    '''\n",
    "    Mapping text data to index matrix\n",
    "    '''\n",
    "    new_tweets = []\n",
    "    for tweet in tweets_list:\n",
    "        new_tweet = []\n",
    "        temp = tweet.replace(\"<user>\", \"\").replace(\"\\n\", \"\").replace(\"<url>\", \"\").split()\n",
    "        for word in temp:\n",
    "            try:\n",
    "                new_tweet.append(p_new_dic[word]) \n",
    "            except:\n",
    "                new_tweet.append(0)  # Set to 0 if not present in the vocabulary\n",
    "        new_tweets.append(new_tweet)\n",
    "    return np.array(new_tweets,dtype=object)   \n",
    "\n",
    "def text_cut_to_same_long(tweets_list):\n",
    "    '''\n",
    "    Cut the data to the same specified length  \n",
    "    '''\n",
    "    data_num = len(tweets_list)\n",
    "    new_ = np.zeros((data_num,maxlen)) \n",
    "    se = []\n",
    "    for i in range(len(tweets_list)):\n",
    "        new_[i,:] = tweets_list[i,:maxlen]        \n",
    "    new_ = np.array(new_, dtype=object)\n",
    "    return new_\n",
    "    \n",
    "def creat_wordvec_tensor(embedding_weights,X_T):\n",
    "    '''\n",
    "    Map the index matrix into a word vector matrix\n",
    "    '''\n",
    "    X_tt = np.zeros((len(X_T),maxlen,vocab_dim))\n",
    "    num1 = 0\n",
    "    num2 = 0\n",
    "    for j in X_T:\n",
    "        for i in j:\n",
    "            X_tt[num1,num2,:] = embedding_weights[int(i),:]\n",
    "            num2 = num2+1\n",
    "        num1 = num1+1\n",
    "        num2 = 0\n",
    "    return X_tt\n",
    "\n",
    "def creat_wordvec_mean_tensor(embedding_weights,X_T):\n",
    "    '''\n",
    "    Map the index matrix into a mean word vector matrix\n",
    "    '''\n",
    "    X_tt = np.zeros((len(X_T),vocab_dim))\n",
    "    num1 = 0\n",
    "    num2 = 0\n",
    "    for j in X_T:\n",
    "        temp = np.zeros((vocab_dim,))\n",
    "        for i in j:\n",
    "            temp += embedding_weights[int(i),:]\n",
    "            num2 = num2+1\n",
    "        if num2 == 0:\n",
    "            X_tt[num1,:] = temp\n",
    "        else:\n",
    "            X_tt[num1,:] = temp/num2\n",
    "        num1 = num1+1\n",
    "        num2 = 0\n",
    "    return X_tt\n",
    "\n",
    "# helper function to show an image\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2SwBdY_X-tmb",
   "metadata": {
    "id": "2SwBdY_X-tmb"
   },
   "outputs": [],
   "source": [
    "vocab_dim = 20 \n",
    "maxlen = 25  # Maximum length of text retention\n",
    "   \n",
    "embedding_weights = np.load(\"embeddings.npy\") \n",
    "# Set a zero vector for words that do not appear in the vocabulary\n",
    "embedding_weights = np.r_[np.zeros((1, vocab_dim)),embedding_weights]\n",
    "\n",
    "f = open(\"vocab.pkl\", 'rb') \n",
    "index_dict = pickle.load(f)    # index dictionary {'word': idx}\n",
    "\n",
    "# Index each word + 1 because of the zero vector\n",
    "for key, value in index_dict.items():  \n",
    "    index_dict[key] = value + 1 \n",
    "\n",
    "with open(\"../twitter-datasets/train_neg.txt\", \"r\", encoding='UTF-8') as f:\n",
    "    neg_data = f.readlines()\n",
    "with open(\"../twitter-datasets/train_pos.txt\", \"r\", encoding='UTF-8') as f:\n",
    "    pos_data = f.readlines()\n",
    "    \n",
    "data = neg_data + pos_data\n",
    "\n",
    "label_list = ([0] * len(neg_data) + [1] * len(pos_data))\n",
    "\n",
    "\n",
    "train_x = text_to_index_array(index_dict, data)\n",
    "train_y = np.array(label_list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "mqz4OF_2_JGX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqz4OF_2_JGX",
    "outputId": "d4b42351-bc59-4ea9-e422-57fae369f562"
   },
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# # Cut the data to the same specified length \n",
    "# train_x = pad_sequence([torch.from_numpy(np.array(x)) for x in train_x],batch_first=True).float() \n",
    "# train_x = text_cut_to_same_long(train_x)\n",
    "\n",
    "# # Index to vector\n",
    "# train_x = creat_wordvec_tensor(embedding_weights,train_x)\n",
    "\n",
    "# train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "# train_loader = DataLoader(train_data, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88f8dd32-69fd-4a4e-8ced-277f36489710",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_x = creat_wordvec_mean_tensor(embedding_weights,train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6c8d6-15f7-488a-a08b-9d3334cfd436",
   "metadata": {
    "id": "aef6c8d6-15f7-488a-a08b-9d3334cfd436"
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter('Glove_vec_visual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4HOlGIb7DNUF",
   "metadata": {
    "id": "4HOlGIb7DNUF"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "\n",
    "# select random images and their target indices\n",
    "word, labels = select_n_random(torch.from_numpy(train_x), np.array(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "nWoS3dR3KOca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nWoS3dR3KOca",
    "outputId": "17ba65b5-0860-484c-fbb6-76f2219e0cfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# get the class labels for each image\n",
    "classes = (':(', ':)')\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "writer.add_embedding(word,\n",
    "                    metadata=class_labels)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qLvj28VXCvmf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLvj28VXCvmf",
    "outputId": "d7e97a48-0009-49b7-a776-7a66b90b8d8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-07 15:39:21.290725: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.9.1 at http://127.0.0.1:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir=Glove_vec_visual --host=127.0.0.1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
