{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOUJ9D/bQAsxXgFU8Su6whK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ot69iVdwh1dT"},"outputs":[],"source":["# Reference of this code is: https://github.com/LSIR/DIS/blob/master/Exercises/week%209/text-classification-transformers_sol.ipynb\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torchtext.data import get_tokenizer\n","from torch.utils.data import Dataset, DataLoader\n","from torchtext.datasets import IMDB\n","from collections import Counter, OrderedDict\n","from torchtext.vocab import vocab, GloVe\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from sklearn.metrics import precision_recall_fscore_support\n","import csv"]},{"cell_type":"markdown","source":[],"metadata":{"id":"wNdrMhJcKXXg"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ir_F2X-Di_qS","executionInfo":{"status":"ok","timestamp":1670705255715,"user_tz":-60,"elapsed":1754,"user":{"displayName":"李心蔚","userId":"16297085280211430075"}},"outputId":"8a29dce9-0c15-4b61-ffd6-3751c1dab872"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["pos_file_path = \"/content/drive/MyDrive/Colab Notebooks/twitter-datasets/train_pos.txt\"\n","neg_file_path = \"/content/drive/MyDrive/Colab Notebooks/twitter-datasets/train_neg.txt\"\n","test_file_path = \"/content/drive/MyDrive/Colab Notebooks/twitter-datasets/test_data.txt\"\n","pos_file_path_prep = \"/content/drive/MyDrive/Colab Notebooks/twitter-datasets/train_pos_prep.txt\"\n","neg_file_path_prep = \"/content/drive/MyDrive/Colab Notebooks/twitter-datasets/train_neg_prep.txt\"\n","test_file_path_prep = \"/content/drive/MyDrive/Colab Notebooks/twitter-datasets/test_data_prep.txt\"\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/output/'"],"metadata":{"id":"GgEcZWzKixjr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load training data\n","# with open(pos_file_path, encoding='utf8') as f:\n","with open(pos_file_path_prep, encoding='utf8') as f:\n","    content = f.readlines()\n","data = [x.strip() for x in content]\n","data_pos = pd.DataFrame(data)\n","\n","# with open(neg_file_path, encoding='utf8') as f:\n","with open(neg_file_path_prep, encoding='utf8') as f:\n","    content = f.readlines()\n","data = [x.strip() for x in content]\n","data_neg = pd.DataFrame(data)\n","\n","data_pos['label'] = 1\n","data_neg['label'] = 0\n","data_all = pd.concat([data_neg,data_pos], axis=0)\n","data_all = data_all.sample(frac=1).reset_index()\n","data_all = data_all.rename(columns = {0:'tweet'})\n","data_all = data_all.drop(['index'],axis=1)\n","\n","# Load submission data\n","# with open(test_file_path, encoding='utf8') as f:\n","with open(test_file_path_prep, encoding='utf8') as f:\n","    content = f.readlines()\n","data = [x.strip() for x in content]\n","submission_data = pd.DataFrame(data)\n","\n","submission_data = submission_data.rename(columns = {0:'tweet'})\n","submission_data['label'] = 1\n","\n","training_size = int(0.9 * len(data_all))"],"metadata":{"id":"4n08OkAGh5Xe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create dataset\n","class TextClassificationDataset(Dataset):\n","    def __init__(self, data, tokenizer, text_vocab, label_vocab, split='train'):\n","        print(f'Numericalising tokens for {split} set...', end=\"\", flush=True)\n","        self.dset = []\n","        self.labels = []\n","        self.text_vocab = text_vocab\n","        self.label_vocab = label_vocab\n","        for tweet, label in data:\n","            tokens = tokenizer(tweet.rstrip())\n","            self.dset.append([self.text_vocab[w] for w in tokens])\n","            self.labels.append(self.label_vocab[str(label)])\n","        print(f'Number of {split} samples: {len(self.dset)}')\n","\n","    def __len__(self):\n","        return len(self.dset)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.dset[idx]\n","        label = self.labels[idx]\n","        return tokens, label\n","\n","    def tokens(self):\n","        return self.text_vocab.get_itos()\n","\n","    def vocab_size(self):\n","        return len(self.text_vocab)\n","\n","    def num_classes(self):\n","        return len(self.label_vocab)\n","\n","    @classmethod\n","    def build_vocab(cls, data, tokenizer, split='train', min_freq=1, pad_token='<pad>', unk_token='<unk>'):\n","        print(f'Building vocab for twitter...', end=\"\", flush=True)\n","        tokens = []\n","        labels = []\n","        for tweet, label in data:\n","            tokens += tokenizer(tweet.rstrip())\n","            labels.append(str(label))\n","\n","        def create_vocab(counts, mf=1):\n","            counter = Counter(counts)\n","            sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n","            ordered_dict = OrderedDict(sorted_by_freq_tuples)\n","            return vocab(ordered_dict, min_freq=mf)\n","\n","        text_vocab = create_vocab(tokens, min_freq)\n","        # set index for padding token\n","        text_vocab.insert_token(pad_token, 0)\n","        # set default index for unknown token\n","        text_vocab.append_token(unk_token)\n","        text_vocab.set_default_index(text_vocab[unk_token])\n","        label_vocab =  create_vocab(labels)\n","        print('Done!')\n","        print(f'Number of tokens: {len(text_vocab)}')\n","        print(f'Classes: {label_vocab.get_stoi()}')\n","        return cls(data, tokenizer, text_vocab, label_vocab, split)\n","    \n","seed = 123\n","torch.manual_seed(seed)\n","tokenizer = get_tokenizer(\"basic_english\")"],"metadata":{"id":"fWreWIz9h8yX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data=data_all[0:training_size]\n","test_data=data_all[training_size:]\n","train_set = TextClassificationDataset.build_vocab(train_data.values, tokenizer, split='train', min_freq=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gV4DfNGSo3xf","executionInfo":{"status":"ok","timestamp":1670704897301,"user_tz":-60,"elapsed":7790,"user":{"displayName":"李心蔚","userId":"16297085280211430075"}},"outputId":"01dc22e8-8f5b-47e2-f78a-3b20550bf05e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Building vocab for twitter...Done!\n","Number of tokens: 31622\n","Classes: {'1': 1, '0': 0}\n","Numericalising tokens for train set...Number of train samples: 180000\n"]}]},{"cell_type":"code","source":["vec = GloVe(name='6B', dim=100)\n","embeddings = vec.get_vecs_by_tokens(train_set.tokens(), lower_case_backup=True)"],"metadata":{"id":"N-JfLSLKiEcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AttentionModel(nn.Module):\n","    def __init__(\n","            self,\n","            vocab_size,\n","            num_label,\n","            e_dim=100,\n","            num_layer=1,\n","            num_head=2,\n","            dropout=0.1,\n","            max_len=512,\n","            padding_idx=0,\n","            weights=None,\n","            freeze=True,\n","            device=torch.device('cuda:0')):\n","\n","        super().__init__()\n","        self.padding_idx = padding_idx\n","        self.max_len = max_len\n","        if weights is None:\n","            self.word_embeddings = nn.Embedding(vocab_size, e_dim, padding_idx=padding_idx)\n","            if freeze:\n","                self.word_embeddings.weight.requires_grad = False\n","        else:\n","            self.word_embeddings = nn.Embedding.from_pretrained(weights, freeze=freeze, padding_idx=padding_idx)\n","        self.position_embeddings = nn.Embedding(max_len, e_dim)\n","        self.position_ids = torch.arange(max_len).to(device)\n","        self.LayerNorm = nn.LayerNorm(e_dim, eps=1e-5)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        transformer_layer = nn.TransformerEncoderLayer(d_model=e_dim, nhead=num_head, dim_feedforward=e_dim * 4)\n","        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_layer)\n","        self.classifier = nn.Linear(e_dim, num_label)\n","        self.device = device\n","        self.to(device)\n","\n","    def forward(self, inputs):\n","        token_ids, attn_masks = inputs\n","        batch_size = token_ids.size(0)\n","        batch_max_length = token_ids.size(1)\n","        # compute embeddings\n","        token_embs = self.word_embeddings(token_ids)\n","        pos_embs = self.position_embeddings(self.position_ids[:batch_max_length])\n","        embeddings = token_embs + pos_embs\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        # compute contextualised embeddings with transformer\n","        contextualised_embs = self.transformer(embeddings.permute(1, 0, 2), src_key_padding_mask=attn_masks)\n","        outputs = contextualised_embs.mean(0)\n","        logits = self.classifier(outputs)\n","        return logits\n","\n","    def knn(self, token_ids, k=10):\n","        query = self.word_embeddings.weight[token_ids]\n","        x_src = F.normalize(query)\n","        x_tgt = F.normalize(self.word_embeddings.weight)\n","        # compute cosine similarity\n","        scores = x_src @ x_tgt.t()\n","        top_values, top_indices = torch.topk(scores, k + 1)\n","        return top_indices[:, 1:]  # remove top1 since it is the target token\n","\n","    def collate_batch(self, batch):\n","        label_list, text_list, lengths = [], [], []\n","        for (_text, _label) in batch:\n","            label_list.append(_label)\n","            text_list.append(_text)\n","            lengths.append(len(_text))\n","        max_length = min(max(lengths), self.max_len)\n","        # truncate or add padding to the right hand side\n","        for i, _text in enumerate(text_list):\n","            if len(_text) < max_length:  # pad\n","                text_list[i] += [self.padding_idx] * (max_length - len(_text))\n","            else:  # truncate\n","                text_list[i] = _text[:max_length]\n","        label_list = torch.tensor(label_list, dtype=torch.long).to(self.device)\n","        text_list = torch.tensor(text_list, dtype=torch.long).to(self.device)\n","        attn_mask = text_list == self.padding_idx\n","        return label_list, (text_list, attn_mask)\n","\n","model = AttentionModel(\n","    vocab_size=train_set.vocab_size(), \n","    num_label=train_set.num_classes(), \n","    e_dim=100,         \n","    max_len=512,\n","    num_head=2,\n","    num_layer=1,\n","    weights=embeddings,\n","    freeze='store_true',\n","    padding_idx=train_set.text_vocab['<pad>'],    \n","    device=torch.device('cuda:0'))\n","   # device=torch.device('cpu'))\n","\n","@torch.no_grad()\n","def word_knn(model, valid_set, vocab, top=10):\n","    print('--------------------------------------------------------------------------------')\n","    print('Validation - top 10 nearest neighbors ------------------------------------------')\n","    valid_token_ids = vocab.lookup_indices(valid_set)\n","    top_indices = model.knn(valid_token_ids, top)\n","\n","    for i, word in enumerate(valid_set):\n","        results = ' '.join([vocab.lookup_token(top_indices[i, k].item()) for k in range(top)])\n","        print(f'{word}: {results}')\n","    print('--------------------------------------------------------------------------------')\n","\n","\n","# define the loss function\n","ce_loss = nn.CrossEntropyLoss()\n","\n","# define the optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","train_dataloader = DataLoader(train_set, \n","                            batch_size=512,                 #\n","                            shuffle=True, \n","                            collate_fn=model.collate_batch)\n","\n","\n","# loop over training epochs\n","EPOCHS = 60\n","for epoch in range(1, EPOCHS + 1):\n","    pbar = tqdm(train_dataloader)\n","    pbar.set_description(\"[Epoch {}]\".format(epoch))\n","    for labels, inputs in pbar: \n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = ce_loss(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        # display the loss \n","        pbar.set_postfix(loss=loss.item())\n","\n","\n","\n","# validation\n","test_set = TextClassificationDataset(\n","    test_data.values,\n","    tokenizer, \n","    text_vocab=train_set.text_vocab, \n","    label_vocab=train_set.label_vocab,\n","    split='test')\n","\n","test_dataloader = DataLoader(test_set, \n","                            batch_size=512,              \n","                            shuffle=False, \n","                            collate_fn=model.collate_batch)\n","\n","y_true = []\n","y_pred = []\n","\n","with torch.no_grad():\n","    model.eval()\n","    for labels, inputs in tqdm(test_dataloader, desc='[Validation]'):\n","        logits = model(inputs)\n","        y_pred += logits.argmax(dim=1).tolist()\n","        y_true += labels.tolist()\n","\n","pre, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n","\n","print('------- Evaluation metrics --------')\n","print(f'Precision: {pre*100:.2f}%')\n","print(f'Recall: {rec*100:.2f}%')\n","print(f'F1 score: {f1*100:.2f}%')\n","print('-' * 35)\n"],"metadata":{"id":"1eiRkF2oiL_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create classification for test data\n","submission_set = TextClassificationDataset(\n","    submission_data.values,\n","    tokenizer, \n","    text_vocab=train_set.text_vocab, \n","    label_vocab=train_set.label_vocab,\n","    split='train')\n","\n","submission_dataloader = DataLoader(submission_set, \n","                            batch_size=256, \n","                            shuffle=False, \n","                            collate_fn=model.collate_batch)\n","\n","y_pred=[]\n","with torch.no_grad():\n","    model.eval()\n","    for labels, inputs in tqdm(submission_dataloader, desc='[Testing]'):\n","        logits = model(inputs)\n","        y_pred += logits.argmax(dim=1).tolist()\n"],"metadata":{"id":"I0hJcUuu0qrr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670700887986,"user_tz":-60,"elapsed":503,"user":{"displayName":"李心蔚","userId":"16297085280211430075"}},"outputId":"d8e7ee0d-2609-4a6d-a586-4f9502df8c64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Numericalising tokens for train set...Number of train samples: 9997\n"]},{"output_type":"stream","name":"stderr","text":["[Testing]: 100%|██████████| 40/40 [00:00<00:00, 285.33it/s]\n"]}]},{"cell_type":"code","source":["# create submission\n","def create_csv_submission(ids, y_pred, name):\n","    \"\"\"\n","    Creates an output file in .csv format for submission\n","    Arguments: ids (event ids)\n","          y_pred (predicted labels)\n","          name (string name of .csv output file to be created)\n","    \"\"\"\n","    with open(name, 'w') as csvfile:\n","        names = ['Id', 'Prediction']\n","        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=names)\n","        writer.writeheader()\n","        for r1, r2 in zip(ids, y_pred):\n","            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n","\n","# y_pred = np.array(y_pred) * 2 - 1\n","y_pred = np.array(y_pred) * (-2) + 1\n","y_pred = list(y_pred)\n","# create_csv_submission(np.arange(1, len(y_pred)+1), y_pred, output_dir+'submission_transformer.csv')\n","create_csv_submission(np.arange(1, len(y_pred)+1), y_pred, output_dir+'submission_transformer_prep.csv')\n","y_pred"],"metadata":{"id":"ghR6PxjWlWPq"},"execution_count":null,"outputs":[]}]}